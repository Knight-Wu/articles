 分布式系统的设计
可以参考es 如何基于lucene 构建分布式搜索引擎的
单机存储随着业务的增长会遇到性能与单点故障问题。通常有两种解决方案：

* 数据分布

就是把数据分块存在不同的服务器上（分库分表）, 解决单机性能问题. 常见的有hash 分布, 范围分布

* 数据复制

让所有的服务器都有相同的数据，提供相当的服务, 一是容错, 二是提供读的并发, 三是可以多个副本可以提供更近的数据.
有主从复制, 和对等复制. es 数据复制由写入节点并发向多个副本写入, 多个副本能扩展读的能力

* cap 理论

在理論計算機科學中，CAP定理（CAP theorem），又被稱作布魯爾定理（Brewer's theorem），它指出對於一個分布式计算系統來說，不可能同時滿足以下三點：[1][2]

一致性（Consistency） （等同于所有节点访问同一份最新的数据副本）
可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据）
分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择[3]。）
根據定理，分佈式系統只能滿足三項中的兩項而不可能滿足全部三項[4]。理解CAP理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。

* 两阶段提交

如果第二阶段提交时, 有节点回复失败, 要回滚, 然后此时有部分节点又回滚失败了呢? 多个节点数据不一致, 只要大多数节点是一致的就是可用的? 

* lucene 写入过程

1. 先写memotable , 做校验, 然后再写log 记录操作, 保证数据多份, 然后返回客户端成功
2. memotable 达到一定大小持久化成segment(这个过程叫refresh), 最后刷到磁盘之后才清空log
3. 要形成segment 之后才能够做全文搜索, 这时候才构建索引, 否则只能根据docId 搜索memotable. docId 只在单个shard 内唯一, 多个shard 有可能重复
4. segment 也是append only 的模式, 不能改的, 类似LSM, segment 数量太多会合并 merge, 小文件会影响性能. 

* es 数据分布

index 可以理解为用于业务上的定义一批数据, routing 字段用于数据在多个shard 分布, 默认hash, routing 字段默认是doc_id.
shard 可能包含多个或一个(合并后)lucene segment, shard 可以理解为es 对lucene segment 的封装, 只能存在于单台机器, 多个shard 就组成了一个分布式的存储, 解决性能问题. 

* es 数据一致性如何保证

A "primary shard" is the main home for a document. A "replica shard" is a copy of the primary shard that provides (1) failover in case the primary dies and (2) increased read throughput
