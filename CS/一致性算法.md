* 文章
https://m.aliyun.com/yunqi/articles/690624?spm=a2c4e.11155435.0.0.3fe07a1a265rNi

#### paxos 的证明
由以下步骤证明而成. 
1. 一个acceptor 接受proposer (单点失败)
2. 每个 acceptor 只接受一个proposer , 可能有相同数量的acceptor 接受了两个不同的proposer , 例如偶数个数的acceptor, 或者奇数个数的acceptor 挂了一个.
3. 每个acceptor  接受多个propose, propose 由一个独特的id: n 和内容: val 组成.两个 propose  , id肯定不同, val 可能相同, 则被多数acceptor 接受的 value 则被accept, 
4. 如何保证3 呢, P2: If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v, 这里就看不懂了
先别看证明了, 先看这个算法提出后, 解决了什么问题, 这个算法在应用中, 在选举过程中如何保证一定能选出一个意见领袖

#### paxos 过程
分为proposer，acceptor， learner 暂时不用管它

* prepare 阶段
proposer  发出一个message , 只包含一个独特的编号为 n, 若acceptor 接受过的message 的编号称为m, 若m 小于n , 则将编号m和 value Mv( 统称为promise) 返回给proposer, 并承诺不会再响应小于n 的message ; 若m >= n , 则回复一个 Nack 给proposer, 让他停止.

* accept 阶段
若 proposer 在上一个阶段收到半数以上的回复, 则可以提出一个propose, 但是这个propose 不是任意的, 若之前acceptor 返回的Mv 均为null ,则可以提出该proposer 原本想提出的value, 否则用收到的promise 中具有最大的编号的vaule 作为提出的 value(为了达成一致 ), 接下来 accptor 会收到 accept 请求, 包含n 和v , 当且仅当 之前accpetor 承诺不再响应小于n 的message, 那么会接受此propose, 并发送accepted 信息给proposer 和learner, 假设此时有其他大于n 的message 收到, 则会拒绝 accept

#### 疑问
* Even with just two proposed values, if each is accepted by
about half the acceptors, failure of a single acceptor could make it impossible to learn which of the values was chosen (如何避免)

### raft
 1. raft 用在什么地方
 多台机器需要达成一致和共识的地方, 向一个集群写入数据, 一个 leader, 多个 follower, 如何保证数据以整个集群为整体看来是一致的, 而不是以客户端的角度看来是一致的, 写入成功的数据后续肯定能读到, 哪怕 leader 发生了切换. 但是返回客户端失败也有可能实际写入成功. 例如五台机器, 只复制了 leader 和一个 follower, 如果返回给 client 前, leader 挂了, 那么新的 leader 如果是这台 follower 数据会比客户端认为的要新, 可以认为是只有复制到大多数节点才保证提交, 但是整个集群数据也是一致的. 


 常见的分布式可能导致不一致性的问题如何解决, 
 a. 写入返回客户端成功之后, leader 挂掉  
 因为是大多数节点返回成功才最终返回 client 成功, 而且选择新 leader 选择的是有最多最新数据的节点, 所以依然能保证一致性.
 b. 返回客户端成功之前, leader 挂掉, 那么客户端会收到失败, 而可以继续尝试. 如何避免重复请求呢, 保证满足串行化语义呢, 给每个 client 分配一个 client id, 并且让 client 每个请求申请一个 uuid, leader 检测到重复就丢弃. 
 c. 如果只是单个 follower 失去去 leader 的联系, 转为 candidate, 并 increase own term , 再发送 requestVote 给其他节点, 但是只要其他节点与 leader 的append rpc 没有超时, 则不会投票给这个 follower, 更加证明这是一个强 leader 的系统. 
 d. 如何保证 client 每次都获取到最新的数据呢, 假设请求刚转发到旧 leader, 就发生了新旧 leader 的切换, 旧 leader 数据已经是旧的了, 例如新的客户端请求, 由于 leader 地址是缓存的有时效的, 可能访问到旧 leader, 那如何避免呢, 可以让 leader 要确保返回最新的数据之前联系一下大多数节点, 保证自己是当前的 leader; 或者可以维持一个和多数节点通过 append rpc 更新的一个 lease 租约 , 假设选举超时是 1s, leader 广播到所有节点并接受返回是 20-50ms, 那么可以设置 lease 超时时间为: 旧leader 与大多数节点通信之后是 0s,  选出新 leader 是 1s, 那么lease 就要小于 1s , 如果超过 1s 还没与大多数节点做过一次 append, 则认为 lease 超时

 2. 怎么选择最新, 最多数据的候选者作为新的 leader 呢?
 与旧 leader 心跳失败的 follower 都会变为 candidate, 然后发送 requestVote rpc 到其他 follower, 获得大多数票的 cadidate 变为新的 leader , 请求中带上candidate 的 term, 最新的日志的 term 和 index , 其他接收请求的 follower 如果自己的日志比 candidate 的新, 或者 term 更大, 则不投票, 否则投票; 得到大多数投票的 follower 或者接收到新的 leader 的 append 请求(只要新的 leader 的 term id 大于自己的)就自动变为 follower.为避免大家同时开始选举自己作为新的 leader, 超时选举这个参数是随机生成的, 各个 follower 不同. 

3. leader 把日志同步到follower, 如果失败会一直重试, 只要日志复制到大多数节点之后, 就可以被认为是提交了. 如果新的 leader 和 follower 的日志冲突, 则会在 append rpc 检测, 并用 leader 的日志重写, 因为每个日志都有个 index 和 term id, 可以认为是顺序一致的, 只要一个相同之后, 前面的日志都可以认为相同, 并从后面再用 leader 的日志覆盖即可. 

4. 如何保证每次新选的 leader 都拥有之前提交过的所有日志呢
因为每次 candidate 都要联系大多数机器后才会成为新的 leader, 而之前提交过的日志肯定存在于大多数机器中的某一台, 因为任意两个大多数的集合肯定有一个交集, 所以只要 candidate 只要能选为 leader 那么他肯定是有所有提交过的日志的. 
5. snapshot 
保存所有日志, 而不打包是很浪费空间的, 从故障中恢复到内存中也更难. raft 的策略是每台节点当 log size 达到一定大小都可以独立的 install snapshot, 而不是只能从 leader 获取 snapshot, 因为在正常的副本中, log entry 跟得上所以没必要, 从本地 snapshot 也提高了性能, leader 发送 snapshot 到follower 通常出现在落后或者新加入的节点. 