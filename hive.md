

#### 与关系型数据库和hbase 的区别
> 关系型数据库是基于写时模式, 数据校验发生在数据写入的时候, 并伴随着索引等数据结构的更新, 会比较慢, 但是查询的时候会比较快; hive是基于读时模式, 检验数据格式的操作是在读取的时候进行校验. 而且hive的数据不是结构化的, 难以预料到具体的数据格式和存储模式.

> hive不支持关系数据库的对某行数据的操作(主要是修改), 只支持对数据的追加和覆盖, 也不支持事务和索引.

> hive进行查询的时候很慢, hbase在查询方面的性能比较强, 但是不支持类sql的查询, 可以嵌入 phoenix on hbase.

#### hive 配置优先级
> 1. Hive SET 命令, 例如 hive > SET hive.enforce.bucketing=true
> 2. 命令行 -hiveconf, 例如 hive > hive -hiveconf fs.default.name=localhost -hiveconf key=val
> 3. hive-site.xml
> 4. hive-default.xml
> 5. hadoop-site.xml
> 6. hadoop-default.xml

#### hive架构
* hive --service help
> 可以获取hive 服务列表


#### hive执行流程
参考 [https://cwiki.apache.org/confluence/display/Hive/Design](https://cwiki.apache.org/confluence/display/Hive/Design)
1. Parser
> 将sql转化为解析树

2. Semantic Analyser
> 将解析树转化为内在的查询表达式, 包括列名检查, 类型检查, 隐式类型转换, 如果是分区表, 则过滤不需要的分区, 如果有采样, 则也过滤

3. Logical Plan Generator
> 将内部的查询表达式转化为逻辑计划,逻辑计划由一系列的operator组成, 可以理解成函数吧,  这过程还包括一系列的优化, 例如group-by, 提前在map端进行聚合, 这段话有待理解: performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key

* Optimizer
> 基于规则的优化, 例如包括列裁剪, 预测下推(可以理解为filter表达式提前, )

> 也可以改成基于代价的优化, 比较不同的优化, 看哪个的性能更高, The optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775)
> A correlation optimizer was added in Hive 0.12.



4. Query Plan Generator
> 将逻辑执行计划转化为 MR任务,


#### hive on spark 
> Hive operator plan from semantic analyzer is further translated a task plan that Spark can execute(语法解析器生成的执行计划将被转化为spark 能够执行的)


##### hive level design
* 优点
> spark 用户后续可以用到hive的新特性

* query planning
   > hive 通过semantic analyzer生成的逻辑计划, 包括以下操作符: TableScanOperator, ReduceSink, FileSink, GroupByOperator, etc, 由 SparkCompiler编译生成可以在spark上面跑的计划


#### hive 手动依赖第三方jar 
1：启用hive-cli时候，使用–auxpath选项指定要添加的jar包，多个jar包以逗号分隔：

./hive –auxpath /tmp/lxw1234/lucene-core-3.6.0.jar,/tmp/lxw1234/IKAnalyzer2012_u6.jar,/tmp/lxw1234/udf.jar

2：进入hive-cli之后，使用ADD JAR命令添加jar包（推荐）：

ADD JAR file:///tmp/lxw1234/lucene-core-3.6.0.jar;

ADD JAR file:///tmp/lxw1234/IKAnalyzer2012_u6.jar;

ADD JAR file:///tmp/lxw1234/udf.jar;
如果jar包放在hdfs上，则路径前面不需要加文件系统标识。

特别注意：这两种方式添加jar包时候，需要注意jar包的顺序，比如上面udf.jar中引用了IKAnalyzer2012_u6.jar中的类，而IKAnalyzer2012_u6.jar中引用了lucene-core-3.6.0.jar中的类，必须按照依赖顺序添加，否则还是会找不到类。


### hive 优化
#### Dynamic file compaction
> 用于调整文件数过多, 进行一次自动的压缩

> By setting the parameters shown below, Hive will add an extra conditional step at the end of the sequence of Map-Reduce jobs. This step calculates the average size of the files generated by the job and will run an automatic compaction if they are smaller than a certain threshold.The compaction step is not “free”.  It can take some time to execute and it will use cluster resources. 
The total time and resources used will depend on the amount of data generated.Nevertheless, it may be better for performance to ensure that data is already generated in larger files, rather than wait to compact them later. Consider using this option is the queries are generated a very large number of small files (hundreds or more).  These parameters dynamically assess the need for compaction and the ideal number of the compacted files.

```
# Enable conditional compaction for map-only jobs

SET hive.merge.mapfiles = true;

# Enable conditional compaction for map-reduce jobs

SET hive.merge.mapredfiles = true;

# Target size for the compacted files – this is a target,
# not a hard limit. Leave a buffer between this number 
# and 256 MB (268435456 bytes)

SET hive.merge.size.per.task = 256000000;

# Average size threshold for file compaction – the compaction 
# will only execute if the average file size is smaller 
# than this value

SET hive.merge.smallfiles.avgsize = 134217728;
```

* Forced file compaction(指定reducer的数量)
> Another way to force file compaction is to specify the number of reducers to be used for a Hive query.  Since each reducer will generate an output file, the number of reducers can be used as a proxy to control the number of generated files.

>Because of the factors above, this method should only be used when you know, as least roughly, how much data the query is producing. This technique is especially useful when the query always returns a very small amount of data (less than 256 MB) that can be easily handled by a single reducer.

>If the Hive query is a Map-Only query the parameters above will have no effect. In this case,modify the query adding a SORT BY 1 clause to force a reduce phase for the query.
