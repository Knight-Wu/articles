### 脚本类
* 每天统计的小文件脚本
已发邮件给陈杰, 目前部署在10.176.229.168 /app/group1/yx_analyse_fsimage 
用crontab 运行, 

* 统计top 10 增量目录的脚本
部署在 10.176.166.11 /app/group2/wutong/increment_file , crontab 运行, 因为邮件的http 服务只能在166.11 调用, 若小文件脚本报错导致没有生成数据, 则邮件内容为空, 
正常邮件内容: 
异常邮件内容: 
当出现异常邮件时, 需要先检查小文件脚本是否报错. 

* 批量清理hdfs 目录的脚本
部署在10.176.229.168 /app/group1/yx_analyse_fsimage/cleanTempDir
用crontab 运行, 目前每天批量删除的目录, 只删除十四天以前的, 包括: hive staging dir, hive logs, hive srcatch dir

### 代码类
* 解析cloudera manager 生成的monitor json 数据, 并落地hive
代码需要注意的事项以及部署代码的步骤都写在了注释里, 见 ImportJsonToHive.scala , 另一个java 版本的可忽略. 

* 介绍一下有关json 解析的其他方法
1. 验证json 格式
[https://stackoverflow.com/questions/42385036/validate-json-file-syntax-in-shell-script-without-installing-any-package](https://stackoverflow.com/questions/42385036/validate-json-file-syntax-in-shell-script-without-installing-any-package)

例如
```
cat YOURFILENAME | python -c "import sys,json;json.loads(sys.stdin.read());print 'OK'"
```

2. 简单json 直接导入hive
a. 首先, 若字段没有非法字段, 则可以用
[https://github.com/quux00/hive-json-schema](https://github.com/quux00/hive-json-schema)
直接生成json 对应的hive schema, 若出现数组为空的异常, 可以修改一下源码看看. 
b. 然后, 可以用这个来解析 [https://github.com/rcongiu/Hive-JSON-Serde](https://github.com/rcongiu/Hive-JSON-Serde), 或者根据hive 官网的指导 [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)

### hadoop 经验
一些问题我总结在了我的博客: 
[https://github.com/Knight-Wu/articles/tree/master/big%20data](https://github.com/Knight-Wu/articles/tree/master/big%20data)

1. commonTree exception (升级到新集群会解决这个问题): 
[https://github.com/Knight-Wu/articles/blob/master/big%20data/Exception:%20Spark%20driver%20throw%20CommonTree%20ClassNotFoundEx%2C%20Executor%20throw%20block%20not%20have%20enough%20number%20of%20replicas.md](https://github.com/Knight-Wu/articles/blob/master/big%20data/Exception:%20Spark%20driver%20throw%20CommonTree%20ClassNotFoundEx%2C%20Executor%20throw%20block%20not%20have%20enough%20number%20of%20replicas.md), 
有关hdfs pipeline recover, lease recover 等 recovery 可以在[https://github.com/Knight-Wu/articles/blob/master/big%20data/hadoop-hdfs.md](https://github.com/Knight-Wu/articles/blob/master/big%20data/hadoop-hdfs.md) 搜索 'recover'

2. 集群之前的一些优化
[https://github.com/Knight-Wu/articles/blob/master/big%20data/cdh%20cluster%20optimizing.md](https://github.com/Knight-Wu/articles/blob/master/big%20data/cdh%20cluster%20optimizing.md)


that 's all
> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTM2ODYwMTg2NiwtMTUxMTQ2NjY4MSwtMT
YzMDE2MTU5NSwtMjA1NTk1MzQ5NV19
-->