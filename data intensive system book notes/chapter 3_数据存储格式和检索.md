**一开始引入的问题是如何存储 key val,**
# 最简单的模型, hash index and append only file
hash index: key, value(实际val 在文件中字节的偏移量)
append file: 一开始一直往一个文件里面 append, append 到一定大小, 进行文件 compaction, 只保留最新一个 key 的 val, 形成一个 segment, 然后可以在后台线程进行多个 segments 的合并, 合并完成之后再切换读和写, 并删除合并前的文件. 

* crash recovery

可以存储 hash index 的 snapshot 在磁盘上, 避免 crash 之后重新扫描文件, 否则崩溃恢复是不可接受的, 非常慢. 

* 如何删除某个key

在hash index 上更新val 为特殊标记

* 如何防止磁盘空间用满呢

按照一定大小组织磁盘的文件, 超过就新建一个文件, 然后定期合并, 相同key 保留更新的val, 然后删除旧的文件

* 为什么不直接在 file 进行 update key, 而是append key, 然后再后台线程进行合并和去重呢? (why append only file ? )

因为就算在 SSD 随机读写性能也不好, append only file 很好的利用了这个特性, update key 会造成随机读写. 

* 缺陷
1. hash index 如果不能完全放在内存中, 磁盘随机 IO 会很慢
2. hash index 不能友好的支持范围查询


# SSTables( Sorted string table) and LSM-Trees( Log-Structured Merge-Tree)
* LSM 核心思想

即使有许多微妙的东西，LSM树的基本思想 —— 保存一系列在后台合并的SSTables —— 简
单而有效。即使数据集比可用内存大得多，它仍能继续正常工作。由于数据按排序顺序存
储，因此可以高效地执行范围查询（扫描所有高于某些最小值和最高值的所有键），并且因
为磁盘写入是连续的，所以LSM树可以支持非常高的写入吞吐量。

* SSTables 即 key 是排序的, 跟上面的 hash index 相比好处在哪呢, 

1. 排序之前, 两个 file segments 进行合并, 一个有 n 行, 另一个有 m 行, 合并多个重复的 key, 合并成一个 segment 的时间复杂度是 O(m*n), 排序之后是归并排序, 是 O(m+n)
2. index 不需要保留每个 key, 只需要能断定 key 所在的范围, 再查找
3. 因为一个 index key 能得到一组 key, 所以可以把这一组 key 在存入磁盘前进行压缩. 
那如何使插入的 key 是有序的呢, 使用红黑树和跳表等数据结构,一般都使用跳表作为内存结构，leveldb等 

* 现有的 storage engine 描述如下


1. 写入 key , val 首先到 memTable(红黑树等数据结构)
2. 当 memTable 膨胀到一定程度, 将有序的 key, val, 写入到磁盘 SSTable( 每个 SSTable 对应一个 sparse index(稀疏 index) ? )
3. 读请求的时候, 先查找 memTable, 再按时间顺序倒序查找 SSTable
4. 查找 SSTable 的时候因为key 是分段保存的, 简单来说某个有序的文件只需要保存最大key 和最小key, 然后搜索某个key 先匹配到某个文件, 再在具体文件中二分查找 key
4. 在后台周期性合并 SSTables, 因为同一个时间内只写一个memotable, 然后映射成一个有序的文件, 所以多个文件的key 时间是不一样的不会出现冲突, 保留更新的key即可
5. 为了防止 memTable 的数据在未写入磁盘的时候崩溃, 需要先写 write ahead log
6. 如果查找不存在的key, 在现有的数据结构下代价会很高, 需要先查找 memTable, 因为刚写入的在这里, 按照顺序查, 最新的key 查到了就直接返回, 查不到再依次查找SSTable, 可以使用布隆过滤器, 迅速判断某一个 key 不存在. 
</br>
思想: 一次写操作由一次顺序 IO(WAL log append)和一次内存写就能完成, 大大提升了写性能, 写包括更新和删除, 这两个都是在内存中记一个标记, 待后续合并的时候就知道数据被更新了.

* 为什么一个sstable 文件只需要一个简单的索引呢, 例如最大最小key.


因为你无论如何都要把相关联的key 读到内存中, 在内存中遍历一个几千字节的文件是很快的, 远快于在磁盘中找到这个文件. 

* WAL 日志何时被丢弃

当把memotable 持久化到sstable 时就可以

* 查找不存在的key时间复杂度很高, 要扫描很多文件

可以用bloom 过滤器
# 列式存储
有时候查询只需要一行中的某几列, 若用行存储, 则需要把拥有几百列的几行全部查出来才做过滤, 但是列存储, 把每一列都按照相同行顺序放在一个文件里. (parquet 就是一种列式存储)

列存储还有个好处就是同样的缓存例如cpu的，可以容纳更多的数据

* 列压缩( 96 页 )
适用于列中不同值的数量远小于行数,  将这一列的每一个不同值用位图编码, 每个不同的值分别用一个 bit 数组表示, 如果这个 bit 数组中零很多, 也可以用游程编码(run-length encoded), 进一步减小存储空间.  
例如 id 列 1, 2 , 3, 9
name 列: a, b, c, d
1 这个值对应 bit 数组 [1, 0, 0, 0], 游程编码: 1 个 one, 3 个 zero
2 对应 bit 数组 [0, 1, 0, 0]
a 对应 bit 数组 [1, 0, 0, 0]
where name = "a" and id=2, 将 [0, 1, 0, 0] 与 [1, 0, 0, 0] 按位与, 结果等于 1 的行即返回, 大大减少了存储和传输的带宽.

# B tree VS LSM 

是在磁盘上维护有序结构的数据结构, key 也是按顺序排列的, 
![image](https://user-images.githubusercontent.com/20329409/212450002-c9c56069-71e2-4f5e-a80a-9cd8d836243d.png)

分支因子为 500 的 4KB 页面的四级树可以存储多达 256TB, 分支因子是分叶子节点的子节点数量.

* 修改

 修改需要读取到叶子节点, 并拿到整个page 并更新; 删除就如果page 已经清空了就需要一些合并之类的; 新建key 的话如果page 满了就需要分割成两个一半的page .
 由于B树写要修改原来的page势必并发的时候有竞争，可以采取写时复制的方式，但也还是不适合大规模的写入，适合读多写少的场景，lsm适合写多读少，


* 与LSM 区别

这个是原地修改page , LSM 是不修改, 只append, 然后合并

* B 树 IO 次数, vs B 树

    * 如何计算B 树的IO 次数呢
 因为B+ 树叶子节点和非叶子节点存储的指针数量不一样, B+ 树不好计算, 时间复杂度跟page 大小无关, 虽然一般16 KB, 但是不懂能存几行数据, 每行的大小未知. 所以一般假设每个page 能存 B 行数据, 那么总共有 N 行, 总的B 树的高度就是 logB N, 以B 为底, 类比二叉树, 就是 (logN) / (logB), 
例如
Say you have a billion rows, and you can currently fit 64 keys in a node. Then the depth of the tree is (log 109)/ log 64 ≈ 30/6 = 5. Now you rebuild the tree with keys half the size and you get log 109 / log 128 ≈ 30/7 = 4.3. Assuming the top 3 levels of the tree are in memory, then you go from 2 disk seeks on average to 1.3 disk seeks on average, for a 35% speedup.
    * 如何计算B+ 树存储的行数呢
    通常三层可以存储千万条数据, 假设每个page 16 KB, 每行1KB, 叶子节点存 16 行, 根节点保留在内存, 三层总条数: 16 * 1000 * 1000 = 一千六百万行, 四层就160 亿行, 保留两层在内存 16MB, 三层 16GB. 
    
</br> 
B+Tree 非叶子节点上是不存储数据的，仅存储键值，数据存储在同一层的叶节点，而B-Tree节点中不仅存储键值，也会存储数据。之所以这么做是因为在数据库中页的大小是固定的，innodb中页的默认大小是16KB。如果不存储数据，那么就会存储更多的键值，相应的树的阶数(节点的子节点树)就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的IO次数有会再次减少，数据查询的效率也会更快。另外，B+Tree的阶数是等于键值的数量的，如果B+Tree一个节点可以存储1000个键值，那么4层(包括根节点)B+树可以存储1000×1000×1000=10亿个数据。一般根节点是常驻内存的，所以一般我们查找10亿数据，只需要3次磁盘IO, 每读一个page 算一次磁盘IO, 所以具体几次取决于有几层是放在磁盘上的. 

* WAL , write ahead log 也叫 redo log

用于崩溃恢复, 区别于undo log , 用于事务回滚




* 叶子节点持有左右的指针就可以不用跳回父节点就可以用来做范围查询

* 写入放大

B树只改几个字节也要写两次，一次写wal一次写整个page，所以page不能太大因为你每次都要写整个page，太小的话同样层次就存不了这么多数据
写入放大b树更大，要写整个page，而lsm只用写该写的东西


* LSM 缺点

Lsm 的缺点就是写入和合并同时进行，在写入很大的时候合并速率慢会占用磁盘空间，且读文件多，合并速率快又影响正常写入
且一个数据可能存在多个地方，事务实现难？为什么，只要有undolog，回滚的时候再写一遍新的值不就好了？
而B树的行为又相对能预测, 而B树的事务能直接找到key所在的page

* 索引
聚簇索引和非聚簇的区别就是实际数据存不存在索引上，非聚簇索引在innodb上是存的主键，再通过主键去聚簇索引里面找行
多列索引，就是把每列的值拼在一起，所以必须按照顺序，且不能中间空一列去查


* 事实表和维度表

事实表中的一些列是属性，例如产品销售的价格和从供应商那里购买的成本(允许计算利润
余额)。事实表中的其他列是对其他表(称为维表)的外键引用。由于事实表中的每一行都
表示一个事件，因此这些维度代表事件的发生地点，时间，方式和原因。


